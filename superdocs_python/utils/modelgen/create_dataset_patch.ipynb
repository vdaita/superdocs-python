{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superdocs_python.utils.diff_utils import parse_diff\n",
    "from datasets import load_dataset, IterableDataset, concatenate_datasets\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from superdocs_python.utils.model import create_model\n",
    "import tiktoken\n",
    "from datasets import Dataset\n",
    "from functools import partial\n",
    "\n",
    "# the prompt for generating is \"Suggest rewrites that...\"\n",
    "#\n",
    "# The output should first be a brief plan and then an execution based on the results\n",
    "#\n",
    "\n",
    "DELIM = \"-----\"\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import re\n",
    "\n",
    "def process_row(row):\n",
    "    udiff = \"\\n\".join(difflib.unified_diff(row['old_contents'].splitlines(), row['new_contents'].splitlines(), fromfile=row['old_file'], tofile=row['new_file'], n=3))\n",
    "    udiff = f\"```diff\\n{udiff}\\n```\"\n",
    "    udiff = re.sub(\"@@.*@@\", \"@@...@@\", udiff)\n",
    "\n",
    "    query = f\"[{row['old_file']}]\\n```\\n{row['old_contents']}```\\nGenerate a unified diff patch that completes the following task: \\n{row['subject']}\"\n",
    "    \n",
    "    return {\"input\": query, \"output\": udiff}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/vijaydaita/.cache/huggingface/modules/datasets_modules/datasets/bigcode--commitpackft/a50f4045a7cb465d763f8230a42419d92af82798b829852b16383b225a647b03 (last modified on Mon Mar 25 14:23:59 2024) since it couldn't be found locally at bigcode/commitpackft, or remotely on the Hugging Face Hub.\n",
      "Downloading data: 100%|██████████| 136M/136M [00:03<00:00, 40.5MB/s] \n",
      "Generating train split: 56025 examples [00:02, 22002.77 examples/s]\n",
      "Downloading data: 100%|██████████| 2.31M/2.31M [00:00<00:00, 5.03MB/s]\n",
      "Generating train split: 100%|██████████| 2199/2199 [00:00<00:00, 87606.26 examples/s]\n",
      "Downloading data: 100%|██████████| 2.21M/2.21M [00:00<00:00, 6.21MB/s]\n",
      "Generating train split: 100%|██████████| 2214/2214 [00:00<00:00, 171509.11 examples/s]\n",
      "Downloading data: 100%|██████████| 567k/567k [00:00<00:00, 2.04MB/s]\n",
      "Generating train split: 100%|██████████| 587/587 [00:00<00:00, 121685.19 examples/s]\n",
      "Downloading data: 100%|██████████| 8.60M/8.60M [00:00<00:00, 10.0MB/s]\n",
      "Generating train split: 100%|██████████| 9337/9337 [00:00<00:00, 272525.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "languages = [\"python\", \"jsx\", \"kotlin\", \"vue\", \"xml\", \"javascript\", \"java\", \"c#\", \"ruby\", \"c\", \"c++\", \"typescript\", \"clojure\", \"dart\"]\n",
    "generated_datasets = []\n",
    "for language in languages:\n",
    "    dataset = load_dataset(\"bigcode/commitpackft\", language, split=\"train\")\n",
    "    dataset = dataset.to_iterable_dataset()\n",
    "    generated_datasets.append(dataset.map(lambda x: process_row(x)))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 56025 examples [00:06, 9281.80 examples/s] \n",
      "Generating train split: 2199 examples [00:00, 8583.81 examples/s]\n",
      "Generating train split: 2214 examples [00:00, 8732.56 examples/s]\n",
      "Generating train split: 587 examples [00:00, 8356.80 examples/s]\n",
      "Generating train split: 9337 examples [00:00, 9528.58 examples/s] \n",
      "Generating train split: 52989 examples [00:06, 8570.74 examples/s]\n",
      "Generating train split: 20635 examples [00:02, 8749.29 examples/s]\n",
      "Generating train split: 9346 examples [00:01, 7789.05 examples/s]\n",
      "Generating train split: 69413 examples [00:07, 8813.88 examples/s]\n",
      "Generating train split: 8506 examples [00:00, 9176.22 examples/s]\n",
      "Generating train split: 4992 examples [00:00, 8984.54 examples/s]\n",
      "Generating train split: 5868 examples [00:00, 8260.53 examples/s]\n",
      "Generating train split: 2403 examples [00:00, 9711.29 examples/s]\n",
      "Generating train split: 765 examples [00:00, 9125.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "generated_regular_datasets = []\n",
    "for generated_dataset in generated_datasets:\n",
    "    ds = Dataset.from_generator(lambda: (yield from generated_dataset), features=generated_dataset.features)\n",
    "    generated_regular_datasets.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 123/123 [00:02<00:00, 56.02ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 123/123 [00:02<00:00, 53.97ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:19<00:00,  9.76s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/vdaita/commitpackft-patches/commit/d6284afb450ed1288851f0cde3eb9a8426023416', commit_message='Upload dataset', commit_description='', oid='d6284afb450ed1288851f0cde3eb9a8426023416', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = concatenate_datasets(generated_regular_datasets)\n",
    "ds.push_to_hub(\"vdaita/commitpackft-patches\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superdocs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
